
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title><p>Language-Conditioned Robotic Manipulation <br/> Fast and Slow Thinking</p></title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

        <!--FACEBOOK-->
    <meta name="og:image" content="https://innermonologue.github.io/img/teaser.png" />
    <meta property="og:image" content="https://innermonologue.github.io/img/teaser.png" />
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="2000">
    <meta property="og:image:height" content="900">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://inner-monologue.github.io/"/>
    <meta property="og:title" content="Inner Monologue: Embodied Reasoning through Planning with Language Models." />
    <meta property="og:description" content="Project page for Inner Monologue: Embodied Reasoning through Planning with Language Models" />

        <!--TWITTER-->
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Inner Monologue: Embodied Reasoning through Planning with Language Models." />
    <meta name="twitter:description" content="Project page for Project page for Inner Monologue: Embodied Reasoning through Planning with Language Models" />
    <meta name="twitter:image" content="https://innermonologue.github.io/img/teaser.png" />


<!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
  <!-- <link rel="icon" type="image/png" href="img/seal_icon.png"> -->
    <!-- Place favicon.ico in the root directory -->

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                <b>Language-Conditioned Robotic Manipulation with Fast and Slow Thinking</br> 
                <!--<small>
                    
                </small>-->
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
               <ul class="list-inline">
                <br>

<li>Minjie Zhu*</li> <li>Yichen Zhu*</li> <li>Jinming Li</li> <li>Junjie Wen</li> <li>Zhiyuan Xu</li> <li>Zhengping Che</li> <br><br><li>Chaomin Shen</li> <li>Yaxin Peng</li> <li>Dong Liu</li> <li>Feifei Feng</li> <li>Jian Tang</li>

              
                <br><br>
                    <!-- <a href="http://g.co/robotics"> -->
                    <!-- <img src="img/robotics-at-google.png" height="40px"> Robotics at Google</a> <br> -->
                    <h5> * Equal contribution. </h5>
                </ul>
            </div>
        </div>

        
        <div class="row">
            <div class="col-md-4 col-md-offset-4 text-center">
                <ul class="nav nav-pills nav-justified">
                    <li>
                        <a href="xxx">
                            <image src="xxx" height="60px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                        <!-- <li>
                            <a href="https://youtu.be/0sJjdxn5kcI">
                                <image src="img/youtube_icon.png" height="60px">
                                    <h4><strong>Video</strong></h4>
                                </a>
                            </li> -->
                            <!-- li>
                                <a href="https://ai.googleblog.com/2021/04/multi-task-robotic-reinforcement.html">
                                    <image src="img/google-ai-blog-small.png" height="60px">
                                        <h4><strong>Blogpost</strong></h4>
                                    </a>
                                </li -->
                                <!-- <li>
                                    <a href="https://github.com/">
                                        <image src="img/github.png" height="60px">
                                            <h4><strong>Code</strong></h4>
                                        </a>
                                    </li> -->
                                </ul>
                                
                            </div>
                        </div>
                        
                        
                        
        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <div class="text-center">
                    <video id="v0" width="100%" playsinline loop controls muted autoplay>
                        <source src="img/im_teaser_compressed.mp4" type="video/mp4">
                    </video>
                </div>
            </div>
        </div> -->

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                                
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
                    The language-conditioned robotic manipulation aims to transfer natural language instructions into executable actions, from simple pick-and-place to tasks requiring intent recognition and visual reasoning. Inspired by the dual-process theory in cognitive science—which suggests two parallel systems of fast and slow thinking in human decision-making—we introduce Robotics with Fast and Slow Thinking (RFST), a framework that mimics human cognitive architecture to classify tasks and makes decisions on two systems based on instruction types. Our RFST consists of two key components: 1) an instruction discriminator to determine which system should be activated based on the current user's instruction, and 2) a slow-thinking system that is comprised of a fine-tuned vision-language model aligned with the policy networks, which allow the robot to recognize user's intention or perform reasoning tasks. 
                    To assess our methodology, we built a dataset featuring real-world trajectories, capturing actions ranging from spontaneous impulses to tasks requiring deliberate contemplation. Our results, both in simulation and real-world scenarios, confirm that our approach adeptly manages intricate tasks that demand intent recognition and reasoning.
                </p>

                <!-- <div class="text-center">
                    <video id="v0" width="100%" playsinline loop controls muted autoplay>
                        <source src="img/im_teaser_compressed.mp4" type="video/mp4">
                   </video>
                </div> -->
            
            </div>
        </div>


        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
               
                <h3>
                    Video Walkthrough
                </h3>
                  <div class="text-center"> -->
                    <!-- <video id="v0" width="100%" playsinline loop controls>
                        <source src="img/im_supp_video_compressed.mp4" type="video/mp4">
                   </video> -->
                    <!-- <div style="position:relative;padding-top:56.25%;">
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/0sJjdxn5kcI" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                </div>
                </div>
            
            </div>
        </div> -->




        <div class="row">
            <div class="col-md-8 col-md-offset-2">
            	<br>
                <h3>
                    Framework
                </h3>
                <p class="text-justify">
                    
                    Dual-process model research indicates that individuals engage with decisions in two primary ways: a rapid, instinctive, subconscious manner (referred to as “System 1 or Fast-thinking”) 
                    and a measured, deliberate, conscious manner (“System 2 or Slow-thinking”). Based on this theory, we propose a framework that mimics human cognitive architecture to classify tasks and makes decisions on two systems based on instruction types.
                    The framework is shown in below.

                <div class="text-center">
                    <image src="img/fst.png" width="80%">
                </div>


            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Experiments
                </h3>
		<p class="text-justify">
            We empirically assess the broad applicability of RFST across diverse tasks in both simulated and real-world settings. 
		</p>

        <h4>
            Experiments on simulator
        </h4>
         <p class="text-justify">
            The Tasks 1 and 2 belong to fast-thinking system, and Task 3-6 belong to slow-thinking system. 
            Our proposed RFST significantly outperforms other methods in accomplishing slow-thinking tasks, achieving notably higher success rates.
        </p>

        <div class="text-center">
                    <image src="img/vima_bench_exp.png" width="80%">
            </div>

        <h4>
            Experiments on real world
        </h4>
        <p class="text-justify">
            The experiments on the real robot. Orange Bars: Slow-thinking tasks. Blue Bars: Fast-thinking tasks. 
            RFST empowers real robots to execute complex tasks such as mathematical reasoning and intent recognition, 
            which were traditionally beyond the scope of conventional robotic manipulation techniques.
        </p>

        <div class="text-center">
                <image src="img/real_exp.png" width="80%">
            </div>

        <h3>
            Video
        </h3>
        
        <div class="text-center">
            <video id="v0" width="80%" playsinline loop controls autoplay muted>
                <source src="img/rfst_demo_v3.mp4" type="video/mp4">
            </video>
        </div>

	    </div>
        </div>
            
     <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <!-- <textarea id="bibtex" class="form-control" readonly="" style="display: none;">@inproceedings{huang2022inner,
title={Inner Monologue: Embodied Reasoning through Planning with Language Models},
author={Wenlong Huang and Fei Xia and Ted Xiao and Harris Chan and Jacky Liang and Pete Florence and Andy Zeng and Jonathan Tompson and Igor Mordatch and Yevgen Chebotar and Pierre Sermanet and Noah Brown and Tomas Jackson and Linda Luu and Sergey Levine and Karol Hausman and Brian Ichter},
booktitle={arXiv preprint arXiv:2207.05608},
year={2022}
}</textarea> -->
            </div>
     </div>

     <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <!-- <p class="text-justify">
The authors would like to thank Kanishka Rao and Vincent Vanhoucke for valuable feedback and discussions. In addition, the authors would like to acknowledge the large team who built SayCan, upon which we construct our Kitchen Mobile Manipulation experiments.
                    <br><br>
                The website template was borrowed from <a href="http://jonbarron.info/">Jon Barron</a>.
                </p>
            </div> -->

    </div>
</body>
</html>
